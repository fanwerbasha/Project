{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><B>Customer Support Analysis</B></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written by Farjana AnwerBasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "with open ('twcs.csv',encoding='utf-8') as dataIn:\n",
    "    reader=csv.DictReader(dataIn)\n",
    "    data={}\n",
    "    for row in reader:\n",
    "        tweetid=int(row['tweet_id'])\n",
    "        data[tweetid]=row['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code will import csv file using 'utf-8' into dictionary data. The 'tweet_id' will be transformed as integer and saved in variable tweetid. This 'tweet_id' will act as a key for the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total Number of Tweets in the data    :  2811774\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\", \"Total Number of Tweets in the data    : \",len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of tweets is derived by checking the length of tweet_id stored in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The content of tweet_id:  269    :  @115770 こんにちは、アマゾン公式です。Fire TV Stickが見れないというのは、どのような状況でしょうか。一般的なトラブルシューティングを記載したヘルプがございますので、ご参照ください。https://t.co/2pbG55qJ7h ET\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\", \"The content of tweet_id:  269    : \",data[269])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tweet_id 269 is being located and the corresponding place value of text being printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tweets contains atleast 50% non-English Characters are   :  20378\n"
     ]
    }
   ],
   "source": [
    "ascii_set=set(\"\"\"!\"#$%&\\'()*+,-./0123456789 :;<=>?@#ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`#abcdefghijklmnopqrstuvwxyz{|}~\"\"\")\n",
    "nonascii_tweetcount=0\n",
    "for key in data:\n",
    "    text_length=len(data[key])\n",
    "    ascii_count=sum(check in ascii_set for check in data[key])\n",
    "    nonascii_count=text_length-ascii_count\n",
    "    if nonascii_count>=ascii_count:\n",
    "        nonascii_tweetcount+=1\n",
    "print(\"\\n\",\"Tweets contains atleast 50% non-English Characters are   : \",nonascii_tweetcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After defining the ascii_set, this code checks every text field and counts every character and stores in 'ascii_count'. non_ascii is bering derived by substracting 'ascii_count' from the length of text. The if statement checks ascii_count with non_ascii_count. If non_ascii_count is 'equal to' or 'greater than', counts of non_ascii_tweet increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total number of Unique username :   725848\n",
      "\n",
      "\n",
      "Top ten usersnames : \n",
      "('@AmazonHelp', 136149)\n",
      "('@AppleSupport', 96688)\n",
      "('@AmericanAir', 48950)\n",
      "('@Uber_Support', 46868)\n",
      "('@Delta', 41084)\n",
      "('@115858', 39001)\n",
      "('@VirginTrains', 37206)\n",
      "('@Tesco', 33565)\n",
      "('@SouthwestAir', 33123)\n",
      "('@SpotifyCares', 30887)\n"
     ]
    }
   ],
   "source": [
    "final_list=[]\n",
    "letter=set('@')\n",
    "for key in data:\n",
    "    word_list=new_list=[]\n",
    "    word_list=data[key].split()\n",
    "    for word in word_list:\n",
    "        if letter & set(word):\n",
    "            new_list.append(word)\n",
    "    user=correct=[]\n",
    "    for word in new_list:\n",
    "        user.append(word[word.find(\"@\")+1:])\n",
    "    correct=['@'+value for value in correct]\n",
    "    final_list.extend(correct)\n",
    "removetable = str.maketrans('', '', '!%?')\n",
    "out_list = [s.translate(removetable) for s in final_list]\n",
    "usercount=Counter(out_list)\n",
    "Toptenuser=Counter(out_list).most_common(10)\n",
    "print(\"\\n\", \"Total number of Unique username :  \",len(usercount)) \n",
    "print(\"\\n\", \"Top ten usersnames : \",*Toptenuser, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After defining '@' into a variable, this code check every row in 'text' for that variable. If words found, it will be appended to another list.  To remove misplaced white spaces before usernames, this code will remove the characters before '@' and '@' itself. After manually adding the '@' symbol on every username, the code 'maketrans' looks for misplaced punctuations like '%,?' and replaces with empty places and save it to final list. Using 'counter', this codes derives the frequency of a username and saves in variable usercount. This code do both 'remove duplicates' and also 'count the frequence of a username' in csv file. By using 'most_common' command the top trending usernames are being displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total number of Unique hashtags :  93242\n",
      "\n",
      "\n",
      "Top ten hashtags : \n",
      "('#mobile_Care', 2066)\n",
      "('#fail', 1543)\n",
      "('#Amazon', 1259)\n",
      "('#hppsdr', 1187)\n",
      "('#AATeam', 1160)\n",
      "('#iPhoneX', 1125)\n",
      "('#help', 1061)\n",
      "('#iOS11', 1048)\n",
      "('#mobile_CareXI', 996)\n",
      "('#azTechHelp', 983)\n"
     ]
    }
   ],
   "source": [
    "final_hashlist=[]\n",
    "for key in data:\n",
    "    letters=set('#')\n",
    "    new_hashlist=hash_list=[]\n",
    "    hash_list=data[key].split()\n",
    "    for word in hash_list:\n",
    "        if letters & set(word):\n",
    "            if len(word)>2:\n",
    "                new_hashlist.append(word)\n",
    "    final_hashlist.extend(new_hashlist)\n",
    "hashcount=Counter(final_hashlist)\n",
    "Toptenhashtags=Counter(final_hashlist).most_common(10)   \n",
    "print (\"\\n\", \"Total number of Unique hashtags : \",len(hashcount))\n",
    "print(\"\\n\", \"Top ten hashtags : \",*Toptenhashtags, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After defining '#' into a variable, this code check every row in 'text' for that variable. If words found, it will be appended to another list. To remove mistyped hashtags like 'empty #, #, , #. and single character #' , this code ignores hashtags whose length is below2 characters. Using 'counter', this codes derives the frequency of a username and saves in variable hashcount. This code do both 'remove duplicates' and also 'count the frequence of a hashtag' in csv file. By using 'most_common' command the top trending hashtags are being displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top ten words used overall : \n",
      "('to', 1686842)\n",
      "('the', 1477191)\n",
      "('you', 1068409)\n",
      "('i', 910205)\n",
      "('a', 884567)\n",
      "('and', 822960)\n",
      "('for', 766013)\n",
      "('your', 705226)\n",
      "('we', 525823)\n",
      "('is', 515218)\n"
     ]
    }
   ],
   "source": [
    "splitlistextend=[]\n",
    "for key in data:\n",
    "    split_list=[]\n",
    "    split_list=data[key].split()\n",
    "    splitlistextend.extend(split_list)\n",
    "    letter=set('@')\n",
    "    letters=set('#')\n",
    "    listat=[]\n",
    "for i in range(0, len(splitlistextend)):\n",
    "    splitlistextend[i] = splitlistextend[i].lower()\n",
    "for word in splitlistextend:\n",
    "    if letter & set(word):\n",
    "        listat.append(word)\n",
    "    if letters & set(word):\n",
    "        listat.append(word)\n",
    "atandhash=Counter(listat) \n",
    "words=Counter(splitlistextend)\n",
    "finaltoptenwords=words-atandhash\n",
    "finaltoptenwords=Counter(finaltoptenwords).most_common(10) \n",
    "\n",
    "print(\"\\n\",\"Top ten words used overall : \",*finaltoptenwords, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data has been splitted and saved into splitlistextend. This code looks for the word contains '@ and #' and append it to 'listat' list. To make 'case insensitive' the list will be converted into lower case using 'lower()'. Then it looks in 'splitlistextend' for the words saved in 'listat' and removes it. . Using 'counter', this codes derives the frequency of a words and saved in variable word. This code do both 'remove duplicates' and also 'count the frequence of a words' in csv file. By using 'most_common' command the top trending words are being displayed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
